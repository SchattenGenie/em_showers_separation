{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'combine_mc_bg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4cb3bcb80915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopera_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine_mc_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_graphx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_x_y_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopera_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBRICK_X_MAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBRICK_X_MIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBRICK_Y_MAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBRICK_Y_MIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAFE_M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'combine_mc_bg'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from tools.opera_tools import combine_mc_bg, gen_graphx, gen_x_y_dataset, load_bg, plot_dataframe\n",
    "from tools.opera_tools import BRICK_X_MAX, BRICK_X_MIN, BRICK_Y_MAX, BRICK_Y_MIN, SAFE_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, log_loss\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from copy import deepcopy      \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def digraph_to_csv(graphs: list):\n",
    "    df = pd.DataFrame(columns=['brick_id', 'shower_id', 'SX', 'SY', 'SZ', 'TX', 'TY'])\n",
    "    for i, graph in tqdm_notebook(enumerate(graphs)):\n",
    "        nodes = graph.nodes()\n",
    "        SX = [node['features']['SX'] for node in nodes.values()]\n",
    "        SY = [node['features']['SY'] for node in nodes.values()]\n",
    "        SZ = [node['features']['SZ'] for node in nodes.values()]\n",
    "        TX = [node['features']['TX'] for node in nodes.values()]\n",
    "        TY = [node['features']['TY'] for node in nodes.values()]\n",
    "        shower_id = [node['signal'] for node in nodes.values()]\n",
    "        brick_id = [i for _ in range(len(shower_id))]\n",
    "        df = df.append(\n",
    "            pd.DataFrame(\n",
    "                {'brick_id': brick_id, \n",
    "                 'shower_id': shower_id, \n",
    "                 'SX': SX, \n",
    "                 'SY': SY, \n",
    "                 'SZ': SZ, \n",
    "                 'TX': TX, \n",
    "                 'TY': TY\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "        \n",
    "\n",
    "def csv_to_digraph(df: pd.DataFrame):\n",
    "    bricks = []\n",
    "    for name, group in df.groupby('brick_id'):\n",
    "        print(group.shape)\n",
    "        nodes_to_add = []\n",
    "\n",
    "        for node_id, row in group.iterrows():\n",
    "            nodes_to_add.append(\n",
    "                (\n",
    "                    node_id,\n",
    "                    {\n",
    "                        'features': {\n",
    "                            'SX': row.SX,\n",
    "                            'SY': row.SY,\n",
    "                            'SZ': row.SZ,\n",
    "                            'TX': row.TX,\n",
    "                            'TY': row.TY,\n",
    "                        },\n",
    "                        'signal': row['shower_id']\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        graphx = nx.DiGraph()\n",
    "        graphx.add_nodes_from(nodes_to_add)\n",
    "        bricks.append(graphx)\n",
    "    return bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e8377dd8f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/opera/opera_train.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brick_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SZ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = np.loadtxt('./data/opera/opera_train.data')\n",
    "df = pd.DataFrame(df, columns=['brick_id', 'SX', 'SY', 'SZ', 'TX', 'TY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будьте осторжны! Не подавайте на вход весь датафрейм\n",
    "plot_dataframe(df.iloc[df.brick_id==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bricks = csv_to_digraph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphx(bricks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYI\n",
    "\n",
    "Оставь надежду всяк сюда входящий\n",
    "\n",
    "Данный код может вам пригодиться если вы захотите запустить MPNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs, sqrt, log\n",
    "def rms_integral_root_closed_py(basetrack_left, basetrack_right, \n",
    "                             TX_LEFT='TX', TY_LEFT='TY',\n",
    "                             TX_RIGHT='TX', TY_RIGHT='TY'):\n",
    "    \"\"\"\n",
    "    Метрика близости между двумя треками\n",
    "    \"\"\"\n",
    "    \n",
    "    dz = basetrack_right['features']['SZ'] - basetrack_left['features']['SZ']\n",
    "    dx = basetrack_left['features']['SX'] - (basetrack_right['features']['SX'] - basetrack_right['features']['TX'] * dz)\n",
    "    dy = basetrack_left['features']['SY'] - (basetrack_right['features']['SY'] - basetrack_right['features']['TY'] * dz)\n",
    "    dtx = (basetrack_left['features']['TX'] - basetrack_right['features']['TX'])\n",
    "    dty = (basetrack_left['features']['TY'] - basetrack_right['features']['TY'])\n",
    "    # dz can be assigned to arbitrary value, acutally !\n",
    "    a = (dtx * dz) ** 2 + (dty * dz) ** 2\n",
    "    b = 2 * (dtx * dz * dx +  dty * dz * dy)\n",
    "    c = dx ** 2 + dy ** 2\n",
    "    if a == 0.:\n",
    "        return fabs(sqrt(c))\n",
    "    discriminant = (b ** 2 - 4 * a * c)\n",
    "    log_denominator = 2 * sqrt(a) * sqrt(a + b + c) + 2 * a + b + EPS\n",
    "    log_numerator = 2 * sqrt(a) * sqrt(c) + b + EPS\n",
    "    first_part = ( (2 * a + b) * sqrt(a + b + c) - b * sqrt(c) ) / (4 * a)\n",
    "    if fabs(discriminant) < EPS:\n",
    "        return fabs(first_part)\n",
    "\n",
    "    result = fabs((discriminant * log(log_numerator / log_denominator) / (8 * sqrt(a * a * a)) + first_part))\n",
    "    return result\n",
    "\n",
    "def class_disbalance_graphx(graphx):\n",
    "    signal = []\n",
    "    for _, node in graphx.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return list(zip(*np.unique(signal, return_counts=True)))\n",
    "\n",
    "def class_disbalance_graphx__(graphx):\n",
    "    signal = []\n",
    "    for _, node in graphx.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return np.unique(signal, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from create_graph.create_graph import generate_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from math import sqrt, log, fabs\n",
    "\n",
    "def scattering_estimation_loss(basetrack_left, basetrack_right):\n",
    "    \"\"\"\n",
    "    Построение некоторой физической фичи. Если страшно, то на функцию лучше не смотреть.\n",
    "    \"\"\"\n",
    "    #if basetrack_right['features']['SZ'] < basetrack_left['features']['SZ']:\n",
    "    #    basetrack_left, basetrack_right = basetrack_right, basetrack_left\n",
    "    \n",
    "    EPS = 1e-6\n",
    "    X0 = 5 * 1000 # mm\n",
    "    Es = 21 # MeV    \n",
    "    \n",
    "    alpha_y = 0\n",
    "    beta_x = 0\n",
    "    beta_y = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    dz = basetrack_right['features']['SZ'] - basetrack_left['features']['SZ']\n",
    "\n",
    "    z = sqrt((basetrack_right['features']['SZ'] - basetrack_left['features']['SZ'])**2 + (basetrack_right['features']['SX'] - basetrack_left['features']['SX'])**2 + (basetrack_right['features']['SZ'] - basetrack_left['features']['SZ'])**2)\n",
    "    theta_x = basetrack_right['features']['TX'] - basetrack_left['features']['TX']\n",
    "    theta_y = basetrack_right['features']['TY'] - basetrack_left['features']['TY']\n",
    "    dx = basetrack_right['features']['SX'] - (basetrack_left['features']['SX'] + basetrack_left['features']['TX'] * dz)\n",
    "    dy = basetrack_right['features']['SY'] - (basetrack_left['features']['SY'] + basetrack_left['features']['TY'] * dz)\n",
    "    \n",
    "    \n",
    "    z_corrected = X0 * (exp(2 * z / X0) - 1)\n",
    "    \n",
    "    alpha_x = 2 * theta_x**2 / (Es**2 * (exp(2 * z / X0) - 1))\n",
    "    alpha_y = 2 * theta_y**2 / (Es**2 * (exp(2 * z / X0) - 1))\n",
    "    \n",
    "    beta_x = 24 * dx**2 / ( Es**2 * X0**3 * (exp(2 * z / X0) - 1)**3)\n",
    "    beta_y = 24 * dy**2 / ( Es**2 * X0**3 * (exp(2 * z / X0) - 1)**3)\n",
    "    \n",
    "    gamma = 2 * (theta_x**2 + theta_y**2)  / (Es**2 * (exp(2 * z / X0) - 1))\n",
    "    \n",
    "    E = sqrt(3 / (alpha_x + alpha_y + beta_x + beta_y + gamma))\n",
    "    \n",
    "    \n",
    "    sigma_theta = Es**2 * (exp(2 * z / X0) - 1) / E**2\n",
    "    sigma_theta_x = sigma_theta / 2\n",
    "    sigma_theta_y = sigma_theta / 2\n",
    "    \n",
    "    sigma_x = Es**2 * (exp(2 * z / X0) - 1)**3 * X0**2 / (48 * E**2)\n",
    "    sigma_y = Es**2 * (exp(2 * z / X0) - 1)**3 * X0**2 / (48 * E**2)\n",
    "    \n",
    "    likelihood = 0.\n",
    "    likelihood -= ( theta_x**2 / (2 * sigma_theta_x) + log(sigma_theta_x) / 2)\n",
    "    likelihood -= ( theta_y**2 / (2 * sigma_theta_y) + log(sigma_theta_y) / 2)\n",
    "    \n",
    "    likelihood -= ( dx**2 / (2 * sigma_x) + log(sigma_x) / 2 )\n",
    "    likelihood -= ( dy**2 / (2 * sigma_y) + log(sigma_y) / 2 )\n",
    "    \n",
    "    \n",
    "    likelihood -= (-log(theta_x**2 + theta_y**2) / 2 + log(sigma_theta) + (theta_x**2 + theta_y**2) / sigma_theta)\n",
    "    return E, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gen_graphx(graphx, layers=2, threshold=250):\n",
    "    graphx_nodes = list(graphx.nodes(data=True))\n",
    "    edges = list(graphx.edges())\n",
    "    graphx.remove_edges_from(edges)\n",
    "    ebunch = generate_distances(graphx_nodes, layers=layers, threshold=threshold)\n",
    "    graphx.add_edges_from(ebunch)\n",
    "    return graphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from operator import itemgetter\n",
    "def k_nearest_cut_succ(graphx, k):\n",
    "    \"\"\"\n",
    "    Функция для поиска k-ближайших соседей-детей в направленном графе.\n",
    "    \"\"\"\n",
    "    for node_id in graphx.nodes():\n",
    "        successors = list(graphx.successors(node_id))\n",
    "        if len(successors) <= k:\n",
    "            continue\n",
    "        edges = list(itertools.product([node_id], successors))\n",
    "        weights = []\n",
    "        for edge in edges:\n",
    "            weights.append(graphx[edge[0]][edge[1]]['weight'])\n",
    "        weights, edges = [list(x) for x in zip(*sorted(zip(weights, edges), key=itemgetter(0)))]\n",
    "        edges_to_remove = edges[k:]\n",
    "        if len(edges_to_remove):\n",
    "            graphx.remove_edges_from(edges_to_remove)\n",
    "    return graphx\n",
    "\n",
    "def k_nearest_cut_pred(graphx, k):\n",
    "    \"\"\"\n",
    "    Функция для поиска k-ближайших соседей-родителей в направленном графе.\n",
    "    \"\"\"\n",
    "    for node_id in graphx.nodes():\n",
    "        predecessors = list(graphx.predecessors(node_id))\n",
    "        if len(predecessors) <= k:\n",
    "            continue\n",
    "        edges = list(itertools.product(predecessors, [node_id]))\n",
    "        weights = []\n",
    "        for edge in edges:\n",
    "            weights.append(graphx[edge[0]][edge[1]]['weight'])\n",
    "        weights, edges = [list(x) for x in zip(*sorted(zip(weights, edges), key=itemgetter(0)))]\n",
    "\n",
    "        edges_to_remove = edges[k:]\n",
    "        if len(edges_to_remove):\n",
    "            graphx.remove_edges_from(edges_to_remove)\n",
    "    return graphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_e_likelihood(graphx):\n",
    "    \"\"\"\n",
    "    Добавление фичей в граф\n",
    "    \"\"\"\n",
    "    for node_left_id, node_right_id, edge in graphx.edges(data=True):\n",
    "        node_left = graphx.node[node_left_id]\n",
    "        node_right = graphx.node[node_right_id]\n",
    "        E, likelihood = scattering_estimation_loss(node_left, node_right)\n",
    "        edge['features'] = {}\n",
    "        edge['features']['r'] = edge['weight']\n",
    "        edge['features']['E'] = E\n",
    "        edge['features']['likelihood'] = likelihood\n",
    "\n",
    "def add_dsxyz(graphx):\n",
    "    \"\"\"\n",
    "    Добавление фичей в граф\n",
    "    \"\"\"\n",
    "    DISTANCE=1293.\n",
    "    for node_id_left, node_id_right, edge in graphx.edges(data=True):\n",
    "        node_left = graphx.node[node_id_left]\n",
    "        node_right = graphx.node[node_id_right]\n",
    "        edge['features']['dsx'] = (node_right['features']['SX'] - node_left['features']['SX']) / DISTANCE\n",
    "        edge['features']['dsy'] = (node_right['features']['SY'] - node_left['features']['SY']) / DISTANCE\n",
    "        edge['features']['dsz'] = (node_right['features']['SZ'] - node_left['features']['SZ']) / DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_for_mpnn(selected_showers, NUM_SHOWERS_IN_BRICK=100, layers=5, threshold=400, k=20, n=20, iters=10):\n",
    "    data = []\n",
    "    for _ in tqdm(range(iters)):\n",
    "        np.random.shuffle(selected_showers)\n",
    "        bricks = gen_bricks(selected_showers, NUM_SHOWERS_IN_BRICK=NUM_SHOWERS_IN_BRICK)\n",
    "        for graphx in bricks:\n",
    "            run_gen_graphx(graphx, layers=layers, threshold=threshold)\n",
    "            add_e_likelihood(graphx)\n",
    "            add_dsxyz(graphx)\n",
    "            k_nearest_cut_succ(graphx, k=k)\n",
    "            k_nearest_cut_pred(graphx, k=n)\n",
    "            connected_components = []\n",
    "            for connected_component in nx.connected_components(nx.Graph(graphx)):\n",
    "                if len(connected_component) > 20:\n",
    "                    connected_components.append(nx.DiGraph(graphx.subgraph(connected_component)))\n",
    "            for i, connected_component in enumerate(connected_components):\n",
    "                class_disbalance = list(zip(*class_disbalance_graphx(connected_component)))[1]\n",
    "                print(class_disbalance)\n",
    "                if min(class_disbalance) > 100 and len(class_disbalance) > 1:\n",
    "                    data.append(connected_component.copy())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = generate_datasets_for_mpnn(selected_showers, NUM_SHOWERS_IN_BRICK=200, layers=5, \n",
    "                                  threshold=400, k=25, n=25, iters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/dataset_mpnn.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
