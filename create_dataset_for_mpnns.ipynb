{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from tools.opera_tools import gen_graphx, gen_x_y_dataset, load_mc\n",
    "from tools.opera_tools import BRICK_X_MAX, BRICK_X_MIN, BRICK_Y_MAX, BRICK_Y_MIN, SAFE_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHOWERS_IN_BRICK = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, log_loss\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from copy import deepcopy      \n",
    "from collections import Counter\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mc(filename=\"mcdata_taue2.root\", step=1):\n",
    "    f = uproot.open(filename)\n",
    "    mc = f['Data'].pandas.df([\"Event_id\", \"ele_P\", \"BT_X\", \"BT_Y\",\n",
    "                              \"BT_Z\",\"BT_SX\", \"BT_SY\",\"ele_x\", \n",
    "                              \"ele_y\", \"ele_z\", \"ele_sx\", \"ele_sy\", \"chisquare\", ], flatten=False)\n",
    "    pmc = pd.DataFrame(mc)\n",
    "    pmc['numtracks'] = pmc.BT_X.apply(lambda x: len(x))\n",
    "    # cuts\n",
    "    shapechange = [pmc.shape[0]]\n",
    "    pmc = pmc[pmc.ele_P > 0.1]\n",
    "    shapechange.append(pmc.shape[0])\n",
    "\n",
    "    pmc = pmc[pmc.ele_z < 0]\n",
    "    shapechange.append(pmc.shape[0])\n",
    "\n",
    "    pmc = pmc[pmc.numtracks > 3]\n",
    "    shapechange.append(pmc.shape[0])\n",
    "    print(\"numtracks reduction by cuts: \", shapechange)\n",
    "    pmc['m_BT_X'] = pmc.BT_X.apply(lambda x: x.mean())\n",
    "    pmc['m_BT_Y'] = pmc.BT_Y.apply(lambda x: x.mean())\n",
    "    pmc['m_BT_Z'] = pmc.BT_Z.apply(lambda x: x.mean())\n",
    "\n",
    "    print(\"len(pmc): {len}\".format(len=len(pmc)))\n",
    "    return pmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numtracks reduction by cuts:  [18724, 18679, 9616, 9106]\n",
      "len(pmc): 9106\n"
     ]
    }
   ],
   "source": [
    "from tools.opera_tools import plot_graphx, DISTANCE, scattering_estimation_loss\n",
    "from tools.opera_tools import gen_graphx, gen_x_y_dataset, load_mc\n",
    "pmc = load_mc(filename='./data/mcdata_taue2.root', step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs, sqrt, log\n",
    "def rms_integral_root_closed_py(basetrack_left, basetrack_right, \n",
    "                             TX_LEFT='TX', TY_LEFT='TY',\n",
    "                             TX_RIGHT='TX', TY_RIGHT='TY'):\n",
    "    \"\"\"\n",
    "    Метрика близости между двумя треками\n",
    "    \"\"\"\n",
    "    \n",
    "    dz = basetrack_right['features']['SZ'] - basetrack_left['features']['SZ']\n",
    "    dx = basetrack_left['features']['SX'] - (basetrack_right['features']['SX'] - basetrack_right['features']['TX'] * dz)\n",
    "    dy = basetrack_left['features']['SY'] - (basetrack_right['features']['SY'] - basetrack_right['features']['TY'] * dz)\n",
    "    dtx = (basetrack_left['features']['TX'] - basetrack_right['features']['TX'])\n",
    "    dty = (basetrack_left['features']['TY'] - basetrack_right['features']['TY'])\n",
    "    # dz can be assigned to arbitrary value, acutally !\n",
    "    a = (dtx * dz) ** 2 + (dty * dz) ** 2\n",
    "    b = 2 * (dtx * dz * dx +  dty * dz * dy)\n",
    "    c = dx ** 2 + dy ** 2\n",
    "    if a == 0.:\n",
    "        return fabs(sqrt(c))\n",
    "    discriminant = (b ** 2 - 4 * a * c)\n",
    "    log_denominator = 2 * sqrt(a) * sqrt(a + b + c) + 2 * a + b + EPS\n",
    "    log_numerator = 2 * sqrt(a) * sqrt(c) + b + EPS\n",
    "    first_part = ( (2 * a + b) * sqrt(a + b + c) - b * sqrt(c) ) / (4 * a)\n",
    "    if fabs(discriminant) < EPS:\n",
    "        return fabs(first_part)\n",
    "\n",
    "    result = fabs((discriminant * log(log_numerator / log_denominator) / (8 * sqrt(a * a * a)) + first_part))\n",
    "    return result\n",
    "\n",
    "def class_disbalance_graphx(graphx):\n",
    "    signal = []\n",
    "    for _, node in graphx.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return list(zip(*np.unique(signal, return_counts=True)))\n",
    "\n",
    "def class_disbalance_graphx__(graphx):\n",
    "    signal = []\n",
    "    for _, node in graphx.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return np.unique(signal, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(pmc, scale = 10000):\n",
    "    \"\"\"\n",
    "    Приводим данные к нормальному разрешению так что нейронкам будет приятнее работать.\n",
    "    \"\"\"\n",
    "    showers = []\n",
    "    for idx in pmc.index:\n",
    "        shower = pmc.loc[idx]\n",
    "        \n",
    "        showers.append(\n",
    "            {\n",
    "                'TX': shower['BT_X'] / scale,\n",
    "                'TY': shower['BT_Y'] / scale,\n",
    "                'TZ': shower['BT_Z'] / scale,\n",
    "                'PX': shower['BT_SX'],\n",
    "                'PY': shower['BT_SY'],\n",
    "                'PZ': np.ones_like(shower['BT_X']),\n",
    "                'ele_P': shower['ele_P'],\n",
    "                'ele_TX': shower['ele_x'] / scale,\n",
    "                'ele_TY': shower['ele_y'] / scale,\n",
    "                'ele_TZ': shower['ele_z']  / scale,\n",
    "                'ele_PX': shower['ele_sx'],\n",
    "                'ele_PY': shower['ele_sy'],\n",
    "                'ele_PZ': 1.\n",
    "            }\n",
    "        )\n",
    "    return showers\n",
    "\n",
    "selected_showers = scale_data(pmc)\n",
    "selected_showers = [selected_shower for selected_shower in selected_showers if len(selected_shower['PX']) > 70]\n",
    "selected_showers = [selected_shower for selected_shower in selected_showers if len(selected_shower['PX']) < 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8019"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_showers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bricks(selected_showers, NUM_SHOWERS_IN_BRICK=25):\n",
    "    \"\"\"\n",
    "    Функция для генерации кирпичей.\n",
    "    \"\"\"\n",
    "    bricks = []\n",
    "    scale = 10000\n",
    "    bricks = []\n",
    "    \n",
    "    X_SIZE = ((BRICK_X_MAX - SAFE_M) / scale / 2) / 2 # default ((BRICK_X_MAX - SAFE_M) / scale / 2)\n",
    "    Y_SIZE = ((BRICK_Y_MAX - SAFE_M) / scale / 2) / 2 # default ((BRICK_Y_MAX - SAFE_M) / scale / 2)\n",
    "    num_processed_showers = 0\n",
    "    while num_processed_showers < len(selected_showers):\n",
    "        to_process_showers = np.random.randint(NUM_SHOWERS_IN_BRICK - 5, NUM_SHOWERS_IN_BRICK + 5)\n",
    "        node_id = 0\n",
    "        nodes_to_add = []\n",
    "        showers_data = []\n",
    "        for j in range(to_process_showers):\n",
    "            num_processed_showers += 1\n",
    "            if num_processed_showers >= len(selected_showers): break\n",
    "            selected_shower = selected_showers[num_processed_showers]\n",
    "            x_diff = np.random.uniform(low=-X_SIZE + 1, high=X_SIZE - 1) - np.median(selected_shower['TX'])\n",
    "            y_diff = np.random.uniform(low=-Y_SIZE + 1, high=Y_SIZE - 1) - np.median(selected_shower['TY'])\n",
    "            showers_data.append(\n",
    "                {\n",
    "                'numtracks': len(selected_shower['PX']),\n",
    "                'signal': j,\n",
    "                'ele_P': selected_shower['ele_P'],\n",
    "                'ele_SX': (selected_shower['ele_TX'] + x_diff) * scale,\n",
    "                'ele_SY': (selected_shower['ele_TY'] + y_diff) * scale,\n",
    "                'ele_SZ': selected_shower['ele_TZ'] * scale,\n",
    "                'ele_TX': selected_shower['ele_PX'] / selected_shower['ele_PZ'],\n",
    "                'ele_TY': selected_shower['ele_PY'] / selected_shower['ele_PZ']\n",
    "                }\n",
    "            )\n",
    "            for k in range(len(selected_shower['PX'])):\n",
    "                nodes_to_add.append(\n",
    "                    (\n",
    "                        node_id,\n",
    "                        {\n",
    "                            'features': {\n",
    "                                'SX': (selected_shower['TX'][k] + x_diff) * scale,\n",
    "                                'SY': (selected_shower['TY'][k] + y_diff) * scale,\n",
    "                                'SZ': selected_shower['TZ'][k] * scale,\n",
    "                                'TX': selected_shower['PX'][k] / selected_shower['PZ'][k],\n",
    "                                'TY': selected_shower['PY'][k] / selected_shower['PZ'][k],\n",
    "                            },\n",
    "                            'signal': j\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                node_id += 1\n",
    "        graphx = nx.DiGraph()\n",
    "        graphx.add_nodes_from(nodes_to_add)\n",
    "        graphx.graph['showers_data'] = showers_data\n",
    "        bricks.append(graphx)\n",
    "    return bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bricks = gen_bricks(selected_showers=selected_showers, NUM_SHOWERS_IN_BRICK=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def digraph_to_csv(graphs: list):\n",
    "    df = pd.DataFrame(columns=['brick_id', 'shower_id', 'SX', 'SY', 'SZ', 'TX', 'TY'])\n",
    "    for i, graph in tqdm_notebook(enumerate(graphs)):\n",
    "        nodes = graph.nodes()\n",
    "        SX = [node['features']['SX'] for node in nodes.values()]\n",
    "        SY = [node['features']['SY'] for node in nodes.values()]\n",
    "        SZ = [node['features']['SZ'] for node in nodes.values()]\n",
    "        TX = [node['features']['TX'] for node in nodes.values()]\n",
    "        TY = [node['features']['TY'] for node in nodes.values()]\n",
    "        shower_id = [node['signal'] for node in nodes.values()]\n",
    "        brick_id = [i for _ in range(len(shower_id))]\n",
    "        df = df.append(\n",
    "            pd.DataFrame(\n",
    "                {'brick_id': brick_id, \n",
    "                 'shower_id': shower_id, \n",
    "                 'SX': SX, \n",
    "                 'SY': SY, \n",
    "                 'SZ': SZ, \n",
    "                 'TX': TX, \n",
    "                 'TY': TY\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "        \n",
    "\n",
    "def csv_to_digraph(df: pd.DataFrame):\n",
    "    bricks = []\n",
    "    for name, group in df.groupby('brick_id'):\n",
    "        print(group.shape)\n",
    "        nodes_to_add = []\n",
    "\n",
    "        for node_id, row in group.iterrows():\n",
    "            nodes_to_add.append(\n",
    "                (\n",
    "                    node_id,\n",
    "                    {\n",
    "                        'features': {\n",
    "                            'SX': row.SX,\n",
    "                            'SY': row.SY,\n",
    "                            'SZ': row.SZ,\n",
    "                            'TX': row.TX,\n",
    "                            'TY': row.TY,\n",
    "                        },\n",
    "                        'signal': row['shower_id']\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        graphx = nx.DiGraph()\n",
    "        graphx.add_nodes_from(nodes_to_add)\n",
    "        bricks.append(graphx)\n",
    "    return bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390be8ba7398459993647d94cc595491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = digraph_to_csv(bricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4322387"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gsplit = GroupShuffleSplit(n_splits=2, test_size=0.3)\n",
    "splits = gsplit.split(df, groups=df.brick_id)\n",
    "df_train, df_test = list(splits)[0]\n",
    "\n",
    "df_train = df.iloc[df_train]\n",
    "df_test = df.iloc[df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3013026"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309361"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.brick_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.brick_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "def scorer(labels_true, labels_pred, groups=None):\n",
    "    if groups is None:\n",
    "        return fowlkes_mallows_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "    fowlkes_mallows = 0.\n",
    "    for group in np.unique(groups):\n",
    "        fowlkes_mallows += fowlkes_mallows_score(labels_true=labels_true[groups==group], \n",
    "                                                 labels_pred=labels_pred[groups==group])\n",
    "    return fowlkes_mallows / len(np.unique(groups))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "# df = pd.read_csv('./opera_all_data.csv')\n",
    "\n",
    "gsplit = GroupShuffleSplit(n_splits=3, test_size=0.3)\n",
    "splits = gsplit.split(df, groups=df.brick_id)\n",
    "train, test = list(splits)[0]\n",
    "\n",
    "df_train = df.iloc[train]\n",
    "df_test = df.iloc[test]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gsplit = GroupShuffleSplit(n_splits=3, test_size=0.5)\n",
    "splits = gsplit.split(df_test, groups=df_test.brick_id)\n",
    "val, test = list(splits)[0]\n",
    "\n",
    "df_val = df.iloc[val]\n",
    "df_test = df.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('opera_train.data', df_train[['brick_id', 'SX', 'SY', 'SZ', 'TX', 'TY']].values, \n",
    "           fmt=['%i', '%5.2f', '%5.2f', '%5.2f', '%1.4f', '%1.4f'])\n",
    "np.savetxt('opera_train.solution', df_train[['brick_id', 'shower_id']].values.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('opera_valid.data', df_val[['brick_id', 'SX', 'SY', 'SZ', 'TX', 'TY']].values, \n",
    "           fmt=['%i', '%5.2f', '%5.2f', '%5.2f', '%1.4f', '%1.4f'])\n",
    "np.savetxt('opera_valid.solution', df_val[['brick_id', 'shower_id']].values.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('opera_test.data', df_test[['brick_id', 'SX', 'SY', 'SZ', 'TX', 'TY']].values, \n",
    "           fmt=['%i', '%5.2f', '%5.2f', '%5.2f', '%1.4f', '%1.4f'])\n",
    "np.savetxt('opera_test.solution', df_test[['brick_id', 'shower_id']].values.astype(int), fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
